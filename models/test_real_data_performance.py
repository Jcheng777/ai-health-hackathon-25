#!/usr/bin/env python3
"""
Enhanced Real Data Performance Testing
Testing both ClinicalBERT + OpenAI and Traditional ML + OpenAI models
"""

import pandas as pd
import numpy as np
import sys
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Import both enhanced models
from clinical_bert_openai_predictor import ClinicalBERTOpenAIPredictor
from openai_denial_predictor import OpenAIDenialPredictor

def load_test_data():
    """Load the real test claims data"""
    try:
        # Try multiple potential locations
        data_paths = [
            '../data/test_claims.csv',
            'data/test_claims.csv',
            '../evaluation/test_claims.csv'
        ]
        
        for path in data_paths:
            if os.path.exists(path):
                print(f"üìä Loading test data from: {path}")
                df = pd.read_csv(path)
                print(f"‚úÖ Loaded {len(df)} test claims")
                return df
        
        print("‚ùå Could not find test_claims.csv")
        return None
        
    except Exception as e:
        print(f"‚ùå Error loading test data: {e}")
        return None

def create_synthetic_test_scenarios():
    """Create comprehensive synthetic test scenarios"""
    scenarios = [
        # Emergency care scenarios
        {
            'Procedure Code': '99282',
            'Diagnosis Code': 'I21.9',
            'Insurance Type': 'Commercial',
            'Billed Amount': 1500,
            'Allowed Amount': 1200,
            'Paid Amount': 1200,
            'expected': 'APPROVED',
            'scenario': 'Emergency care for acute MI'
        },
        {
            'Procedure Code': '99285',
            'Diagnosis Code': 'J44.1',
            'Insurance Type': 'Medicare',
            'Billed Amount': 3500,
            'Allowed Amount': 2800,
            'Paid Amount': 2800,
            'expected': 'APPROVED',
            'scenario': 'High complexity emergency for COPD exacerbation'
        },
        
        # Preventive care scenarios
        {
            'Procedure Code': '99395',
            'Diagnosis Code': 'Z00.00',
            'Insurance Type': 'Commercial',
            'Billed Amount': 350,
            'Allowed Amount': 300,
            'Paid Amount': 300,
            'expected': 'APPROVED',
            'scenario': 'Annual preventive exam adult'
        },
        {
            'Procedure Code': '77057',
            'Diagnosis Code': 'Z12.31',
            'Insurance Type': 'Medicare',
            'Billed Amount': 200,
            'Allowed Amount': 180,
            'Paid Amount': 180,
            'expected': 'APPROVED',
            'scenario': 'Screening mammography'
        },
        
        # Mental health scenarios
        {
            'Procedure Code': '90834',
            'Diagnosis Code': 'F32.9',
            'Insurance Type': 'Commercial',
            'Billed Amount': 150,
            'Allowed Amount': 120,
            'Paid Amount': 120,
            'expected': 'APPROVED',
            'scenario': 'Psychotherapy for major depression'
        },
        {
            'Procedure Code': '90837',
            'Diagnosis Code': 'F33.1',
            'Insurance Type': 'Medicaid',
            'Billed Amount': 200,
            'Allowed Amount': 150,
            'Paid Amount': 150,
            'expected': 'APPROVED',
            'scenario': 'Extended psychotherapy recurrent depression'
        },
        
        # Complex procedures
        {
            'Procedure Code': '27447',
            'Diagnosis Code': 'M17.11',
            'Insurance Type': 'Medicare',
            'Billed Amount': 45000,
            'Allowed Amount': 38000,
            'Paid Amount': 38000,
            'expected': 'APPROVED',
            'scenario': 'Total knee replacement for osteoarthritis'
        },
        {
            'Procedure Code': '33533',
            'Diagnosis Code': 'I25.10',
            'Insurance Type': 'Commercial',
            'Billed Amount': 85000,
            'Allowed Amount': 75000,
            'Paid Amount': 75000,
            'expected': 'APPROVED',
            'scenario': 'Coronary artery bypass surgery'
        },
        
        # Office visits
        {
            'Procedure Code': '99213',
            'Diagnosis Code': 'E11.9',
            'Insurance Type': 'Commercial',
            'Billed Amount': 180,
            'Allowed Amount': 150,
            'Paid Amount': 150,
            'expected': 'APPROVED',
            'scenario': 'Office visit for diabetes management'
        },
        {
            'Procedure Code': '99215',
            'Diagnosis Code': 'I50.9',
            'Insurance Type': 'Medicare',
            'Billed Amount': 400,
            'Allowed Amount': 320,
            'Paid Amount': 320,
            'expected': 'APPROVED',
            'scenario': 'Complex office visit for heart failure'
        },
        
        # Potential denial scenarios
        {
            'Procedure Code': '99215',
            'Diagnosis Code': 'Z51.11',
            'Insurance Type': 'Commercial',
            'Billed Amount': 500,
            'Allowed Amount': 200,
            'Paid Amount': 150,
            'expected': 'DENIED',
            'scenario': 'Excessive billing for routine follow-up'
        },
        {
            'Procedure Code': '27447',
            'Diagnosis Code': 'Z87.891',
            'Insurance Type': 'Self-Pay',
            'Billed Amount': 60000,
            'Allowed Amount': 35000,
            'Paid Amount': 0,
            'expected': 'DENIED',
            'scenario': 'Elective surgery with poor necessity'
        },
        {
            'Procedure Code': '99214',
            'Diagnosis Code': 'M79.3',
            'Insurance Type': 'Commercial',
            'Billed Amount': 800,
            'Allowed Amount': 200,
            'Paid Amount': 100,
            'expected': 'DENIED',
            'scenario': 'Excessive billing for minor complaint'
        }
    ]
    
    return pd.DataFrame(scenarios)

def test_enhanced_traditional_model(test_df, sample_size=100):
    """Test the enhanced Traditional ML + OpenAI model"""
    print("\n" + "="*60)
    print("üî¨ TESTING ENHANCED TRADITIONAL ML + OPENAI MODEL")
    print("="*60)
    
    # Initialize model
    predictor = OpenAIDenialPredictor()
    
    # Train on a subset (simulating training data)
    print("ü§ñ Training enhanced traditional model...")
    train_df = test_df.sample(n=min(200, len(test_df)//2), random_state=42)
    predictor.train_traditional_model(train_df)
    
    # Test on remaining data
    remaining_df = test_df.drop(train_df.index)
    eval_df = remaining_df.sample(n=min(sample_size, len(remaining_df)), random_state=42)
    
    print(f"üìä Testing on {len(eval_df)} claims...")
    
    results = []
    correct_predictions = 0
    confidence_scores = []
    
    for idx, (_, row) in enumerate(eval_df.iterrows()):
        if idx % 20 == 0:
            print(f"  Processed {idx}/{len(eval_df)} claims...")
        
        try:
            # Get enhanced prediction
            prediction_result = predictor.predict_with_enhanced_ensemble(row.to_dict())
            predicted = prediction_result['ensemble_prediction']
            confidence = prediction_result['confidence']
            
            # Determine actual outcome
            actual_outcome = row['Outcome']
            if actual_outcome == 'Denied':
                actual = 'DENIED'
            elif actual_outcome == 'Under Review':
                actual = 'NEEDS_REVIEW'
            else:
                actual = 'APPROVED'
            
            # Check if correct
            is_correct = (predicted == actual) or (predicted == 'NEEDS_REVIEW' and actual in ['DENIED', 'APPROVED'])
            if is_correct:
                correct_predictions += 1
            
            confidence_scores.append(confidence)
            
            # Store detailed results
            results.append({
                'Procedure Code': row['Procedure Code'],
                'Diagnosis Code': row['Diagnosis Code'],
                'Insurance Type': row['Insurance Type'],
                'Billed Amount': row['Billed Amount'],
                'Actual': actual,
                'Predicted': predicted,
                'Confidence': confidence,
                'Correct': is_correct,
                'Weights': prediction_result.get('ensemble_weights', {}),
                'Clinical Reasoning': prediction_result.get('clinical_reasoning', [])[:2]  # First 2 reasons
            })
            
        except Exception as e:
            print(f"Error processing claim {idx}: {e}")
            continue
    
    # Calculate metrics
    accuracy = correct_predictions / len(results) if results else 0
    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
    
    print(f"\n‚úÖ ENHANCED TRADITIONAL ML + OPENAI RESULTS:")
    print(f"   Accuracy: {accuracy:.1%} ({correct_predictions}/{len(results)})")
    print(f"   Average Confidence: {avg_confidence:.3f}")
    
    # Save detailed results
    results_df = pd.DataFrame(results)
    if not results_df.empty:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"../analysis/enhanced_traditional_ml_results_{timestamp}.csv"
        results_df.to_csv(output_file, index=False)
        print(f"   Detailed results saved to: {output_file}")
    
    return results_df, accuracy, avg_confidence

def test_synthetic_scenarios_both_models():
    """Test both models on synthetic scenarios"""
    print("\n" + "="*60)
    print("üß™ TESTING BOTH MODELS ON SYNTHETIC SCENARIOS")
    print("="*60)
    
    scenarios_df = create_synthetic_test_scenarios()
    
    # Initialize both models
    clinical_bert_model = ClinicalBERTOpenAIPredictor()
    traditional_model = OpenAIDenialPredictor()
    
    print(f"Testing {len(scenarios_df)} synthetic scenarios...")
    
    bert_results = []
    traditional_results = []
    
    for idx, row in scenarios_df.iterrows():
        scenario = row['scenario']
        expected = row['expected']
        
        print(f"\nüìã Scenario {idx+1}: {scenario}")
        print(f"   Expected: {expected}")
        
        # Test ClinicalBERT model
        try:
            bert_pred = clinical_bert_model.predict_with_clinical_ai(row.to_dict())
            bert_result = bert_pred['prediction']
            bert_conf = bert_pred['confidence']
            bert_correct = bert_result == expected
            
            print(f"   ClinicalBERT: {bert_result} (conf: {bert_conf:.3f}) {'‚úÖ' if bert_correct else '‚ùå'}")
            
            bert_results.append({
                'Scenario': scenario,
                'Expected': expected,
                'Predicted': bert_result,
                'Confidence': bert_conf,
                'Correct': bert_correct,
                'Model': 'ClinicalBERT + OpenAI'
            })
        except Exception as e:
            print(f"   ClinicalBERT Error: {e}")
        
        # Test Traditional ML model
        try:
            trad_pred = traditional_model.predict_with_enhanced_ensemble(row.to_dict())
            trad_result = trad_pred['ensemble_prediction']
            trad_conf = trad_pred['confidence']
            trad_correct = trad_result == expected
            
            print(f"   Traditional ML: {trad_result} (conf: {trad_conf:.3f}) {'‚úÖ' if trad_correct else '‚ùå'}")
            
            traditional_results.append({
                'Scenario': scenario,
                'Expected': expected,
                'Predicted': trad_result,
                'Confidence': trad_conf,
                'Correct': trad_correct,
                'Model': 'Traditional ML + OpenAI'
            })
        except Exception as e:
            print(f"   Traditional ML Error: {e}")
    
    # Compare results
    bert_accuracy = sum(r['Correct'] for r in bert_results) / len(bert_results) if bert_results else 0
    trad_accuracy = sum(r['Correct'] for r in traditional_results) / len(traditional_results) if traditional_results else 0
    
    print(f"\nüìä SYNTHETIC SCENARIOS COMPARISON:")
    print(f"   ClinicalBERT + OpenAI: {bert_accuracy:.1%}")
    print(f"   Traditional ML + OpenAI: {trad_accuracy:.1%}")
    
    # Save comparison results
    all_results = bert_results + traditional_results
    comparison_df = pd.DataFrame(all_results)
    if not comparison_df.empty:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"../analysis/synthetic_scenarios_comparison_{timestamp}.csv"
        comparison_df.to_csv(output_file, index=False)
        print(f"   Comparison results saved to: {output_file}")
    
    return comparison_df

def main():
    """Main testing function"""
    print("üöÄ ENHANCED MODEL COMPARISON TESTING")
    print("=" * 60)
    
    # Load test data
    test_df = load_test_data()
    if test_df is None:
        print("‚ùå Cannot proceed without test data")
        return
    
    print(f"üìä Loaded {len(test_df)} test claims")
    print(f"   Columns: {list(test_df.columns)}")
    print(f"   Outcomes: {test_df['Outcome'].value_counts().to_dict()}")
    
    # Test synthetic scenarios
    synthetic_results = test_synthetic_scenarios_both_models()
    
    # Test enhanced traditional model on real data
    traditional_results, trad_accuracy, trad_confidence = test_enhanced_traditional_model(test_df, sample_size=100)
    
    # Compare with previous ClinicalBERT results (if available)
    print(f"\nüìà ENHANCED TRADITIONAL ML + OPENAI SUMMARY:")
    print(f"   Real Data Accuracy: {trad_accuracy:.1%}")
    print(f"   Average Confidence: {trad_confidence:.3f}")
    
    print(f"\nüéØ MODEL COMPARISON COMPLETE")
    print(f"   Enhanced Traditional ML + OpenAI model tested successfully")
    print(f"   Results saved in ../analysis/ directory")

if __name__ == "__main__":
    main() 